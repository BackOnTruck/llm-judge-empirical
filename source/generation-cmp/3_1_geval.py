from model_api import Model_API
import json, re
from config import DIR, RESPONDED_DATA, GEVAL_RES, gpt4o, CRITERIA
from concurrent.futures import as_completed, ThreadPoolExecutor
from tqdm import tqdm

#! varies per task
# system prompt; disabled for LLMs not supporting it
system_prompt = "You are an expert software developer."

#! varies per task; evaluation steps generated by 3_1_geval_prep.py
# user prompts; EN template: for LLMs, 1 response; CN template: for humans, 3 responses
template = f"""A developer has implemented the following requirement twice.
You are to compare the quality of the two code snippets, and decide which one is noticeably better or declare a tie because their difference in quality is insignificant.
Note that the developer may provide explanations or comments around the code, which should not affect your judgment of the code.

## Requirement:
```
{{}}
```

## First Implementation:
```
{{}}
```

## Second Implementation:
```
{{}}
```

You should analyze the implementations based on the following aspects:
{CRITERIA}

To evaluate the two implementations, follow these steps:

1. Functional Correctness:
   - Examine the requirement to understand the expected behavior of the code.
   - Analyze each implementation to determine if it faithfully and completely fulfills the requirement.
   - Consider edge cases and error handling in both implementations.
   - Evaluate whether any assumptions made by the developers are justified and if they impact correctness.

2. Readability:
   - Review the naming conventions used in each implementation (variables, functions, classes).
   - Check for clear and logical structure, including consistent indentation and spacing.
   - Look for the use of comments and assess whether they clarify complex sections without being excessive.
   - Assess the use of control structures (loops, conditionals) and whether these contribute to clarity.
   - Evaluate the use of abstractions such as functions or classes that might improve readability.
   - Consider the overall simplicity vs. complexity of the code and how it affects understanding.

Next, apply the analysis to each aspect in turn for both implementations:

1. Provide a summary of the functional correctness for each implementation.
2. Provide a summary of the readability for each implementation.
3. Compare the two implementations across these aspects and decide which is better or if it is a tie.

Now, please fill out the evaluation form by giving the comparison result first by responding "FIRST" "SECOND" or "TIE" to indicate which implementation is better, before comparing them on each aspect and making explanations.
## Evaluation form:
- Overall score: """

N = 20

mapping = {
    'first': 1,
    'tie': 0,
    'second': -1
}
def solve(index: int, query: str, bar: tqdm):
    model = Model_API(system_prompt, gpt4o)
    try:
        replies = model.generate(query, T=1.0, N=N, maxlen=20)
        tqdm.write(f">>> Success: {sum(len(reply) for reply in replies)} characters")

    except Exception as e:
        replies = None
        tqdm.write(f">>> Failed: ({type(e)}) {e}")

    bar.update()
    matches = list(filter(lambda x: x, [re.findall(r'(first|second|tie)', reply.lower()) for reply in replies]))
    if not matches:
        matches = [['tie']]

    return index, model.message, sum(mapping[match[0]] for match in matches) / len(matches), matches

def main():
    with open(f'{DIR}/{RESPONDED_DATA[0]}') as fin:
        batch = []
        lines = list(fin)

        for i in range(0, len(lines), 3):
            samples = [json.loads(line) for line in lines[i:i + 3]]
            for j in range(3):
                for k in range(3):
                    if j != k:
                        #! params vary per task
                        batch += [template.format(samples[0]['input'], samples[j]['output'], samples[k]['output'])]

    with tqdm(total=len(batch), desc='>>> Progress') as bar, ThreadPoolExecutor(max_workers=40) as ex:
        futures = as_completed([ex.submit(solve, idx, sample, bar) for idx, sample in enumerate(batch)])

    path = f'{DIR}/{GEVAL_RES}'
    with open(path, 'w') as fout, open(f'{path}.log', 'w') as flog, open(f'{path}.txt', 'w') as fscores:
        for _, message, score, all_scores in sorted([future.result() for future in futures]):
            print(json.dumps(message), file=flog)
            print(score, file=fout)
            print(json.dumps(all_scores), file=fscores)

if __name__ == '__main__':
    main()
