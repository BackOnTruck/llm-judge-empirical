from model_api import Model_API
import json, re
from config import DIR, RESPONDED_DATA, GEVAL_RES, gpt4o, CRITERIA
from concurrent.futures import as_completed, ThreadPoolExecutor
from tqdm import tqdm

#! varies per task
# system prompt; disabled for LLMs not supporting it
system_prompt = "You are an expert software developer."

#! varies per task; evaluation steps generated by 3_1_geval_prep.py
# user prompts; EN template: for LLMs, 1 response; CN template: for humans, 3 responses
template = f"""A developer has implemented the following requirement.
You are to evaluate the quality of the code.
Note that the developer may provide explanations or comments around the code, which should not affect your judgment of the code.

## Requirement:
```
{{}}
```

## Implementation:
```
{{}}
```

You should analyze the implementation based on the following aspects:
{CRITERIA}

To evaluate the code effectively, you need to follow these steps:

1. Understand the Requirement:
   - First, parse the requirement details to understand what the code is supposed to accomplish.
   - Identify key functionalities and conditions expected from the code.

2. Assess Functional Correctness:
   - Check if the implementation covers all aspects of the requirement.
   - Consider edge cases and see if they are handled correctly.
   - Look for potential errors or bugs that could affect functionality.

3. Evaluate Readability:
   - Examine the code structure, including indentation, comments, and organization.
   - Look at variable and function naming to ensure they are descriptive and meaningful.
   - Assess the use of comments and documentation to see if they clarify the intent without cluttering.

4. Scoring:
   - For each aspect (Functional Correctness and Readability), assign a score based on the predefined criteria.
   - Justify each score with specific observations from the code review.

5. Provide Feedback:
   - Offer constructive feedback and suggestions for improvement.
   - Highlight strengths and areas for enhancement to guide future development efforts.

Now, please fill out the evaluation form by giving the overall score FIRST in the format of `X/5` before scoring each aspect and making explanations.
## Evaluation form:
- Overall score: """

N = 20
def solve(index: int, query: str, bar: tqdm):
    model = Model_API(system_prompt, gpt4o)
    try:
        replies = model.generate(query, T=1.0, N=N, maxlen=20)
        tqdm.write(f">>> Success: {sum(len(reply) for reply in replies)} characters")

    except Exception as e:
        replies = None
        tqdm.write(f">>> Failed: ({type(e)}) {e}")

    bar.update()
    matches = list(filter(lambda x: x, [re.findall(r'([0-5](?:\.\d+)?)', reply) for reply in replies]))
    if not matches:
        matches = [[-1]]

    return index, model.message, sum(float(match[0]) for match in matches) / len(matches), matches

def main():
    with open(f'{DIR}/{RESPONDED_DATA[0]}') as fin:
        batch = []
        for line in fin:
            js = json.loads(line)
            #! params vary per task
            batch += [template.format(js['input'], js['output'])]

    with tqdm(total=len(batch), desc='>>> Progress') as bar, ThreadPoolExecutor(max_workers=40) as ex:
        futures = as_completed([ex.submit(solve, idx, sample, bar) for idx, sample in enumerate(batch)])

    path = f'{DIR}/{GEVAL_RES}'
    with open(path, 'w') as fout, open(f'{path}.log', 'w') as flog, open(f'{path}.txt', 'w') as fscores:
        for _, message, score, all_scores in sorted([future.result() for future in futures]):
            print(json.dumps(message), file=flog)
            print(score, file=fout)
            print(json.dumps(all_scores), file=fscores)

if __name__ == '__main__':
    main()
