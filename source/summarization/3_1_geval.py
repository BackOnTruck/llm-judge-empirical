from model_api import Model_API
import json, re
from config import DIR, RESPONDED_DATA, GEVAL_RES, gpt4o, CRITERIA
from concurrent.futures import as_completed, ThreadPoolExecutor
from tqdm import tqdm

#! varies per task
# system prompt; disabled for LLMs not supporting it
system_prompt = "You are an expert software developer."

#! varies per task; evaluation steps generated by 3_1_geval_prep.py
# user prompts; EN template: for LLMs, 1 response; CN template: for humans, 3 responses
template = f"""A developer has written a summary for the following code.
You are to evaluate the quality of the summary, without considering the quality of the code.

## Code:
```
{{}}
```

## Summary:
```
{{}}
```

You should analyze the summary based on the following aspects:
{CRITERIA}

To evaluate the summary effectively, follow these steps:

1. Readability:

   - Assess the clarity: Is the language straightforward and easy to understand?
   - Determine conciseness: Is the summary brief but comprehensive?
   - Evaluate structure: Is the information logically ordered and well-organized?

2. Consistency:

   - Verify key functionality: Does the summary accurately reflect the main purpose and workings of the code?
   - Identify omissions: Are there any crucial aspects of the code not included in the summary?
   - Check for extraneous details: Does the summary include unnecessary information not supported by the code?

Now, please fill out the evaluation form by giving the overall score FIRST in the format of `X/5` before scoring each aspect and making explanations.
## Evaluation form:
- Overall score: """

N = 20
def solve(index: int, query: str, bar: tqdm):
    model = Model_API(system_prompt, gpt4o)
    try:
        replies = model.generate(query, T=1.0, N=N, maxlen=20)
        tqdm.write(f">>> Success: {sum(len(reply) for reply in replies)} characters")

    except Exception as e:
        replies = None
        tqdm.write(f">>> Failed: ({type(e)}) {e}")

    bar.update()
    matches = list(filter(lambda x: x, [re.findall(r'([0-5](?:\.\d+)?)', reply) for reply in replies]))
    if not matches:
        matches = [[-1]]

    return index, model.message, sum(float(match[0]) for match in matches) / len(matches), matches

def main():
    with open(f'{DIR}/{RESPONDED_DATA[0]}') as fin:
        batch = []
        for line in fin:
            js = json.loads(line)
            #! params vary per task
            batch += [template.format(js['input'], js['output'])]

    with tqdm(total=len(batch), desc='>>> Progress') as bar, ThreadPoolExecutor(max_workers=40) as ex:
        futures = as_completed([ex.submit(solve, idx, sample, bar) for idx, sample in enumerate(batch)])

    path = f'{DIR}/{GEVAL_RES}'
    with open(path, 'w') as fout, open(f'{path}.log', 'w') as flog, open(f'{path}.txt', 'w') as fscores:
        for _, message, score, all_scores in sorted([future.result() for future in futures]):
            print(json.dumps(message), file=flog)
            print(score, file=fout)
            print(json.dumps(all_scores), file=fscores)

if __name__ == '__main__':
    main()
